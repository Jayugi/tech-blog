---
title: Writing to PostgreSQL from Apache Flink®
description: 
tags: Apache Flink®, PostgreSQL
layout: article
---

We started to play around with Apache Flink® to process some of our event data which is generated by execution of Business Processes.
A single execution of a Business Process is called a case, and when a Business Process is executed tasks of the process are being performed.
Every time a task is performed this results in some event data being emitted; these events are collected into the relevant case.
The events of one case form the tracehash of the case.

[Apache Flink®](https://flink.apache.org/ "Apache Flink® at apache.org") is an open-source stream processing framework. 
It is the latest in streaming technology, providing [high throughput with low-latency and exactly once semantics](http://data-artisans.com/high-throughput-low-latency-and-exactly-once-stream-processing-with-apache-flink/ "dataArtisans article introducing the power of Flink").

Something about data artisans, the tutorials and the community?

Link to other blog post I read and used to get started with PostgreSQL sinks?

We use Flink to perform a series of transformations on our data. 
At certain points we want to persist the results to a database (PostgreSQL), from where we serve a REST API.

In order to persist out results to some outside system, we have to use a data sink. 
[Data Sinks](https://ci.apache.org/projects/flink/flink-docs-master/dev/datastream_api.html#data-sinks "Flink data sink documentation") are connectors that consume Data Streams and forward them to files, sockets, external systems, or print them.

Flink provides a number of "out of the box" [connectors](https://ci.apache.org/projects/flink/flink-docs-master/dev/connectors/index.html "out of the box connector documentation") with various guarantees. 
It is also possible to define your own.

## Using the `JDBCOutputFormat`

There is no out of the box PostgreSQL sink for Flink. 
This does not mean, however, that you have to start from scratch! 
The [JDBCOutputFormat](https://github.com/apache/flink/blob/4d27f8f2deef9fad845ebc91cef121cf9b35f825/flink-connectors/flink-jdbc/src/main/java/org/apache/flink/api/java/io/jdbc/JDBCOutputFormat.java "github for JDBCOutputFormat") class can be used to turn any database with a JDBC database driver into a sink.

`JDBCOutputFormat` is/was part of the Flink Batch API, however it can also be used as a sink for the Data Stream API. 
It seems to be the recommended approach, judging from a few discussions I found on the Flink user group.

The `JDBCOutputFormat` requires a prepared statement, driver and database connection.

```java
JDBCOutputFormat jdbcOutput = JDBCOutputFormat.buildJDBCOutputFormat()
     .setDrivername("org.postgresql.Driver")
     .setDBUrl("jdbc:postgresql://localhost:1234/test?user=xxx&password=xxx")
     .setQuery(query)
     .finish();
```

The query is the prepared statement, in our case this is sufficient:

`String query = "INSERT INTO public.cases (caseid, tracehash) VALUES (?, ?)";`

Where our table looks like:

```postgresql
CREATE TABLE cases
(
 caseid VARCHAR(255),
 tracehash VARCHAR(255)
);
```

The `JDBCOutputFormat` can only store instances of Row. 
A Row is a wrapper for the parameters of the prepared statement. 
This means need to transform our data stream of cases into rows. 
We're going to map the id to caseid, and the trace hash to tracehash by implementing a Flink MapFunction.

```java
DataStream<Case> cases = ...
		
		DataStream<Row> rows = cases.map((MapFunction<Case, Row>) aCase -> {
			Row row = new Row(2); // our prepared statement has 2 parameters
			row.setField(0, aCase.getId()); //first parameter is caseid
			row.setField(1, aCase.getTraceHash()); //second paramater is tracehash
			return row;
		});
```

And finally we can specify the sink:

```java
rows.writeUsingOutputFormat(jdbcOutput);
```

So now every time our window is evaluated we get a new row in the database, woo!

However, my console is being spammed with:

`"Unknown column type for column %s. Best effort approach to set its value: %s."`

This is because I did not set explicit type values for my parameters when I built the `JDBCOutputFormat`. I can do so using the builder and simply passing in an array of java.sql.Types.

```java
JDBCOutputFormat jdbcOutput = JDBCOutputFormat.buildJDBCOutputFormat()
     .setDrivername("org.postgresql.Driver")
     .setDBUrl("jdbc:postgresql://localhost:1234/test?user=xxx&password=xxx")
     .setQuery(query)
     .setSqlTypes(new int[] { Types.VARCHAR, Types.VARCHAR }) //set the types
     .finish();
```

And now I don't get spammed with warnings.

Ok, but instead of a new row I want to either create a new row if one does not exist, or update an existing row. I.e. do an upsert.

I'm using PostgreSQL so I will just modify my query to include an `ON CONFLICT` statement.

```java
String query = "INSERT INTO public.cases (caseid, tracehash) VALUES (?, ?) ON CONFLICT (caseid) DO UPDATE SET events=?";
```

This means I have a new parameter, and I must specify a value for this. I need to do so in my MapFunction, which now looks like this:

```java
DataStream<Case> cases = ...
		
DataStream<Row> rows = cases.map((MapFunction<Case, Row>) aCase -> {
	Row row = new Row(3); // our prepared statement has 3 parameters
	row.setField(0, aCase.getId()); //first parameter is caseid
	row.setField(1, aCase.getTraceHash()); //second paramater is tracehash
	row.setField(2, aCase.getTraceHash()); //third parameter is also tracehash
	return row;
});
```

I must also add a type for this parameter when I build the `JDBCOutputFormat`:

```java
JDBCOutputFormat jdbcOutput = JDBCOutputFormat.buildJDBCOutputFormat()
     .setDrivername("org.postgresql.Driver")
     .setDBUrl("jdbc:postgresql://localhost:1234/test?user=xxx&password=xxx")
     .setQuery(query)
     .setSqlTypes(new int[] { Types.VARCHAR, Types.VARCHAR, TypEs.VARCHAR }) //set the types
     .finish();
```

I also need to add a constraint to my table:

```postgresql
CREATE TABLE cases
(
  caseid VARCHAR(255),
  events VARCHAR(255),
  CONSTRAINT cases_unique UNIQUE (caseid)
);
```

This time when I run my job I do not get any two rows with the same caseid.

With this approach, every time I evaluate a case it is mapped to a row and written to the database.
This is a lot of individual writes, what if I want to batch them up, so that I write to PostgreSQL less frequently? 
`JDBCOutputFormat` has a `batchInterval`, which you can specify on the `JDBCOutputFormatBuilder`. If, however, I specify a batch interval of 5000, I would potentially never write anything to the database, or wait a very long time until anything was written. 

Another approach would be to add both a batch interval and a timeout, however there is no easy way to extend `JDBCOutputFormat` to do this, so let's write our own sink.

## Writing our own Sink

In order to write a Sink you must implement `SinkFunction<IN>` where `IN` is the input type parameter.
This was `Row` for our previous sink, this time we can use our `Case` type.

We also have to maintain a database connection and so would like more control over when this is created.
Extending `RichSinkFunction<IN>` means that we get a call back when our function is initialized; this is a good place to set up the database connection.

Firstly, lets just get our sink working without any batching. 
We'll extend `RichSinkFunction<IN>` with the same on conflict prepared statement and database schema as before.
We'll hard code the PostgreSQL driver and connection details.

```java
public class RichCaseSink extends RichSinkFunction<Case> {
	
	private static final String UPSERT_CASE = "INSERT INTO public.cases (caseid, events) "
			+ "VALUES (?, ?) "
			+ "ON CONFLICT (caseid) DO UPDATE SET "
			+ "  events=?";
	
	private PreparedStatement statement;
	
	
	@Override
	public void invoke(Case aCase) throws Exception {
		
		statement.setString(1, aCase.getId());
		statement.setString(2, aCase.getTraceHash());
		statement.setString(3, aCase.getTraceHash());
		statement.addBatch();
		statement.executeBatch();
	}
	
	@Override
	public void open(Configuration parameters) throws Exception {
		Class.forName("org.postgresql.Driver");
		Connection connection =
				DriverManager.getConnection("jdbc:postgresql://localhost:5432/casedb?user=signavio&password=signavio");
		
		statement = connection.prepareStatement(UPSERT_CASE);
	}
	
}
```
We need to add our new sink to our case data stream:

```java
DataStream<Case> cases = ...
cases.addSink(new RichCaseSink());
```

Let's run our job to test out if the sink works, which it does!

To add batching to our sink we'll follow a similar approach to the `JDBCOutputFormat`, but with a timeout.
We'll maintain a count of cases since the last batch was saved and only save if either this count reaches a limit, or a certain time period has passed.

In our invoke function we have to increment a batch count, and only execute a batch if these conditions are met.
Once a batch is executed we have to reset the count and record the time of the last batch.

```java
@Override
public void invoke(Case aCase) throws Exception {
	
	statement.setString(1, aCase.getId());
	statement.setString(2, aCase.getTraceHash());
	statement.setString(3, aCase.getTraceHash());
	statement.addBatch();
	batchCount++;
	
	if (shouldExecuteBatch()) {
		statement.executeBatch();
		batchCount = 0;
		lastBatchTime = System.currentTimeMillis();
	}
}
```

The number of times we write to the database is now reduced, which is great, but it doesn't really mirror how Flink is doing things.

## Checkpoint aware Sink

Flink also has a concept of checkpointing, what about if we wrote then.
What is checkpointing?


